{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the speedup\n",
    "### Extracting District Names into 9 Dictionaries\n",
    "Because there are only nine different scene locations in the downloaded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if it was way more than 9, then could have sorted the file names first, then only the change of scene location, construct a new dictionary ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import ogr, osr, gdal\n",
    "\n",
    "import fiona\n",
    "from shapely.geometry import Point, shape\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change this for Win7,macOS\n",
    "bases = \"C:\\Users\\deepak\\Desktop\\Repo\\Maps\\Districts\\Census\\Dist.shp\"\n",
    "# base_ = \"/Users/macbook/Documents/BTP/Satellite/Data/Maps/Districts/Census_2011\"\n",
    "fc = fiona.open(bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_geocode(pt):\n",
    "    for feature in fc:\n",
    "        if shape(feature['geometry']).contains(pt):\n",
    "            return feature['properties']['DISTRICT']\n",
    "    return \"NRI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base2 = \"G:\\BTP\\Satellite\\Data\\Test\"  # Win7\n",
    "base = \"G:\\BTP\\Satellite\\Data\\Test2\"  # Win7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(os.path.join(base,root)) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s' % root)\n",
    "        tar = tarfile.open(os.path.join(base,filename))\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(os.path.join(base,root))\n",
    "        tar.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LE07_L1GT_146039_20050702_20170115_01_T2 already present - Skipping extraction of LE07_L1GT_146039_20050702_20170115_01_T2.tar.gz\n",
      "LE07_L1GT_146040_20040512_20170120_01_T2 already present - Skipping extraction of LE07_L1GT_146040_20040512_20170120_01_T2.tar.gz\n",
      "LE07_L1GT_146041_20041222_20170117_01_T2 already present - Skipping extraction of LE07_L1GT_146041_20041222_20170117_01_T2.tar.gz\n",
      "LE07_L1GT_147039_20050725_20170114_01_T2 already present - Skipping extraction of LE07_L1GT_147039_20050725_20170114_01_T2.tar.gz\n",
      "LE07_L1GT_147040_20050506_20170116_01_T2 already present - Skipping extraction of LE07_L1GT_147040_20050506_20170116_01_T2.tar.gz\n",
      "LE07_L1GT_147041_20040807_20170119_01_T2 already present - Skipping extraction of LE07_L1GT_147041_20040807_20170119_01_T2.tar.gz\n",
      "LE07_L1GT_148039_20050918_20170113_01_T2 already present - Skipping extraction of LE07_L1GT_148039_20050918_20170113_01_T2.tar.gz\n",
      "LE07_L1GT_148040_20040627_20170120_01_T2 already present - Skipping extraction of LE07_L1GT_148040_20040627_20170120_01_T2.tar.gz\n",
      "LE07_L1GT_149039_20050128_20170117_01_T2 already present - Skipping extraction of LE07_L1GT_149039_20050128_20170117_01_T2.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# extracting for Test2\n",
    "for directory, subdirList, fileList in os.walk(base):\n",
    "    for filename in fileList:\n",
    "        if filename.endswith(\".tar.gz\"): \n",
    "            d = extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories = [os.path.join(base, d) for d in sorted(os.listdir(base)) if os.path.isdir(os.path.join(base, d))]\n",
    "# print directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open(base2 + \"\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B1.TIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "osgeo.gdal.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare one `ds` variable here itself, for the transformation of the coordinate system below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the new coordinate system\n",
    "wgs84_wkt = \"\"\"\n",
    "GEOGCS[\"WGS 84\",\n",
    "    DATUM[\"WGS_1984\",\n",
    "        SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
    "            AUTHORITY[\"EPSG\",\"7030\"]],\n",
    "        AUTHORITY[\"EPSG\",\"6326\"]],\n",
    "    PRIMEM[\"Greenwich\",0,\n",
    "        AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "    UNIT[\"degree\",0.01745329251994328,\n",
    "        AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "    AUTHORITY[\"EPSG\",\"4326\"]]\"\"\"\n",
    "new_cs = osr.SpatialReference()\n",
    "new_cs.ImportFromWkt(wgs84_wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(dsx):\n",
    "    old_cs= osr.SpatialReference()\n",
    "    old_cs.ImportFromWkt(dsx.GetProjectionRef())\n",
    "    trs = osr.CoordinateTransformation(old_cs,new_cs) \n",
    "    return trs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pixel2coord(x, y, xoff, a, b, yoff, d, e):\n",
    "    \"\"\"Returns global coordinates from coordinates x,y of the pixel\"\"\"\n",
    "    xp = a * x + b * y + xoff\n",
    "    yp = d * x + e * y + yoff\n",
    "    return(xp, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts = [[],[],[],[],[],[],[],[],[]]\n",
    "dictr = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "dicti = {\"146039\":0, \"146040\":1, \"146041\":2, \"147039\":3, \"147040\":4, \"147041\":5, \"148039\":6, \"148040\":7, \"149039\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_146039_20050702_20170115_01_T2\\LE07_L1GT_146039_20050702_20170115_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_146040_20040512_20170120_01_T2\\LE07_L1GT_146040_20040512_20170120_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_146041_20041222_20170117_01_T2\\LE07_L1GT_146041_20041222_20170117_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_147039_20050725_20170114_01_T2\\LE07_L1GT_147039_20050725_20170114_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_147040_20050506_20170116_01_T2\\LE07_L1GT_147040_20050506_20170116_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_147041_20040807_20170119_01_T2\\LE07_L1GT_147041_20040807_20170119_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_148039_20050918_20170113_01_T2\\LE07_L1GT_148039_20050918_20170113_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_148040_20040627_20170120_01_T2\\LE07_L1GT_148040_20040627_20170120_01_T2_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test2\\LE07_L1GT_149039_20050128_20170117_01_T2\\LE07_L1GT_149039_20050128_20170117_01_T2_B1.TIF\n",
      "22027.7967823\n",
      "Seconds\n"
     ]
    }
   ],
   "source": [
    "stx = timeit.default_timer()\n",
    "\n",
    "for directory in directories:\n",
    "    \n",
    "    \"\"\" Identifying Month, Year, Spacecraft ID \"\"\"\n",
    "    date = directory.split('\\\\')[-1].split('_')[3] # Change for Win7\n",
    "    satx = directory.split('\\\\')[-1][3]\n",
    "    month = date[4:6]\n",
    "    year = date[0:4]\n",
    "    \n",
    "    scene = directory.split('\\\\')[-1].split('_')[2]\n",
    "    index = dicti[scene]\n",
    "    \n",
    "    #if index != 1: continue\n",
    "    \n",
    "    \"\"\" Visiting every GeoTIFF file \"\"\" \n",
    "    for _,_,files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            \n",
    "            if filename.endswith(\".TIF\"):\n",
    "                \n",
    "                if filename[-5] == '2': break\n",
    "                \n",
    "                print os.path.join(directory,filename)\n",
    "                \n",
    "                ds = gdal.Open(os.path.join(directory,filename))\n",
    "                if ds == None: continue\n",
    "                col, row, _ = ds.RasterXSize, ds.RasterYSize, ds.RasterCount\n",
    "                xoff, a, b, yoff, d, e = ds.GetGeoTransform()\n",
    "                #--------------------\n",
    "                transform = func(ds)\n",
    "                #--------------------\n",
    "                \n",
    "                dictr[index] = (col,row)\n",
    "                \n",
    "                \"\"\" Now go to each pixel, find its lat,lon. Hence its district, and the pixel value \"\"\"\n",
    "                for i in range(0,col,col/k):\n",
    "                    for j in range(0,row,row/k):\n",
    "                        \n",
    "                        ########### fetching the lat and lon coordinates \n",
    "                        x,y = pixel2coord(i, j, xoff, a, b, yoff, d, e)\n",
    "                        lonx, latx, z = transform.TransformPoint(x,y)\n",
    "                        \n",
    "                        \n",
    "                        ########### fetching the name of district\n",
    "                        \n",
    "                        point = Point(lonx,latx)\n",
    "                        district = reverse_geocode(point)\n",
    "                        \n",
    "                        dicts[index].append(district)\n",
    "                        \n",
    "#                         #----------------------------------------------------------\n",
    "#                         if filename[-5] == '1':\n",
    "#                             point = Point(lonx,latx)\n",
    "#                             district = reverse_geocode(point)\n",
    "#                             dicts[i][str(lonx)+str(latx)] = district\n",
    "#                         else:\n",
    "#                             print \"-------------------- !!!!!!! -------------------\"\n",
    "#                             district = dictx[str(lonx)+str(latx)]\n",
    "#                         #----------------------------------------------------------\n",
    "                        \n",
    "            \n",
    "                            \n",
    "elapsed = timeit.default_timer() - stx\n",
    "print (elapsed)\n",
    "print \"Seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7981, 7271), (7741, 7001), (7761, 7021), (7811, 7051), (7801, 7071), (7821, 7051), (7871, 7111), (7861, 7131), (7941, 7181)]\n"
     ]
    }
   ],
   "source": [
    "print dictr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3540173718\n",
      "74.8772487691\n",
      "Hanumangarh\n"
     ]
    }
   ],
   "source": [
    "print latx\n",
    "print lonx\n",
    "print district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n",
      "2601\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print len(dicts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = \"G:\\BTP\\Satellite\\Data\\Bulk\\Landsat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = \"G:\\BTP\\Satellite\\Data\\Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories = [os.path.join(base, d) for d in sorted(os.listdir(base)) if os.path.isdir(os.path.join(base, d))]\n",
    "# print directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File C:\\Users\\deepak\\Desktop\\Repo\\BTP\\Satellite\\Ricep_large.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-ea1be55b6fea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mricep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\Users\\deepak\\Desktop\\Repo\\BTP\\Satellite\\Ricep_large.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# ricep = ricep.drop([\"Unnamed: 0\"],axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ricep[\"value\"] = ricep[\"Production\"]/ricep[\"Area\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mricep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deepak\\Anaconda3\\envs\\ipykernel_py2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deepak\\Anaconda3\\envs\\ipykernel_py2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deepak\\Anaconda3\\envs\\ipykernel_py2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deepak\\Anaconda3\\envs\\ipykernel_py2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\deepak\\Anaconda3\\envs\\ipykernel_py2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File C:\\Users\\deepak\\Desktop\\Repo\\BTP\\Satellite\\Ricep_large.csv does not exist"
     ]
    }
   ],
   "source": [
    "ricep = pd.read_csv(\"C:\\Users\\deepak\\Desktop\\Repo\\BTP\\Satellite\\Ricep_large.csv\")\n",
    "# ricep = ricep.drop([\"Unnamed: 0\"],axis=1)\n",
    "# ricep[\"value\"] = ricep[\"Production\"]/ricep[\"Area\"]\n",
    "ricep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.empty((ricep.shape[0],1))*np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" 'features' contain collumn indexes for the new features \"\"\"\n",
    "\"\"\" 'dictn' is the dictionary mapping name of collumn index to the index number \"\"\"\n",
    "features = []\n",
    "dictn = {}\n",
    "k = 13\n",
    "for i in range(1,13):\n",
    "    for j in range(1,11):\n",
    "        s = str(i) + \"_B\" + str(j) + \"_\"\n",
    "        features.append(s+\"M\")\n",
    "        features.append(s+\"V\")\n",
    "        dictn[s+\"M\"] = k\n",
    "        dictn[s+\"V\"] = k+1\n",
    "        k = k+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,13):\n",
    "    for j in range(1,11):\n",
    "        s = str(i) + \"_B\" + str(j) + \"_\"\n",
    "        features.append(s+\"Mn\")\n",
    "        features.append(s+\"Vn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(index=range(ricep.shape[0]),columns=features)\n",
    "ricex = pd.concat([ricep,tmp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 50\n",
    "hits = 0\n",
    "times = [0,0,0,0,0,0,0,0]\n",
    "nums = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "bx = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B3.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B4.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B5.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B6_VCID_1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B6_VCID_2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B7.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_BQA.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B3.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B4.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B5.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B6_VCID_1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B6_VCID_2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B7.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_BQA.TIF\n",
      "122.503495055\n",
      "Seconds\n"
     ]
    }
   ],
   "source": [
    "stx = timeit.default_timer()\n",
    "\n",
    "for directory in directories:\n",
    "    \n",
    "#     if bx: continue\n",
    "#     else: bx = True\n",
    "    \n",
    "    \"\"\" Identifying Month, Year, Spacecraft ID \"\"\"\n",
    "    date = directory.split('\\\\')[-1].split('_')[3] # Change for Win7\n",
    "    satx = directory.split('\\\\')[-1][3]\n",
    "    month = date[4:6]\n",
    "    year = date[0:4]\n",
    "    \n",
    "    scene = directory.split('\\\\')[-1].split('_')[2]\n",
    "    index = dicti[scene]\n",
    "    \n",
    "    \"\"\" Visiting every GeoTIFF file \"\"\" \n",
    "    for _,_,files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            \n",
    "            \n",
    "            if filename.endswith(\".TIF\"):\n",
    "                \n",
    "                if filename[-5] == '8': continue\n",
    "                \n",
    "                print os.path.join(directory,filename)\n",
    "                \n",
    "                ds = gdal.Open(os.path.join(directory,filename))\n",
    "                if ds == None: continue\n",
    "                col, row, _ = ds.RasterXSize, ds.RasterYSize, ds.RasterCount\n",
    "                xoff, a, b, yoff, d, e = ds.GetGeoTransform()\n",
    "                \n",
    "                ind = -1\n",
    "                for i in range(0,col,col/k):\n",
    "                    for j in range(0,row,row/k):\n",
    "                        \n",
    "                        ind += 1\n",
    "                        if ind>2600: break\n",
    "                            \n",
    "                        st = timeit.default_timer()\n",
    "                        ########### fetching the lat and lon coordinates \n",
    "                        times[0] += timeit.default_timer() - st\n",
    "                        nums[0] += 1\n",
    "                        \n",
    "                        \n",
    "                        st = timeit.default_timer()\n",
    "                        ########### fetching the name of district\n",
    "                        district = dicts[index][ind]\n",
    "                        #----------------------------------------------------------\n",
    "                        times[1] += timeit.default_timer() - st\n",
    "                        nums[1] += 1\n",
    "                        \n",
    "                        if district == \"NRI\": continue\n",
    "                        \n",
    "                        \n",
    "                        st = timeit.default_timer()\n",
    "                        ########### Locating the row in DataFrame which we want to update\n",
    "                        district = district.lower()\n",
    "                        district = district.strip()\n",
    "                        r = ricex.index[(ricex['ind_district'] == district) & (ricex['Crop_Year'] == int(year))].tolist()\n",
    "                        times[3] += timeit.default_timer() - st\n",
    "                        nums[3] += 1\n",
    "                        \n",
    "                        \n",
    "                        if len(r) == 1:\n",
    "                            \n",
    "                            st = timeit.default_timer()\n",
    "                            ########### The pixel value for that location\n",
    "                            px,py = i,j\n",
    "                            pix = ds.ReadAsArray(px,py,1,1)\n",
    "                            pix = int(pix[0][0])\n",
    "                            times[2] += timeit.default_timer() - st\n",
    "                            nums[2] += 1\n",
    "                            \n",
    "                            \n",
    "                            st = timeit.default_timer()\n",
    "                            \"\"\" Found the row, so now ..\"\"\"\n",
    "                            \"\"\" Find Collumn index corresponding to Month, Band \"\"\"\n",
    "                            hits = hits + 1\n",
    "                            #print (\"Hits: \", hits)\n",
    "                            ####### Band Number ########\n",
    "                            band = filename.split(\"\\\\\")[-1].split(\"_\")[7:][0].split(\".\")[0][1]\n",
    "                            bnd = band\n",
    "                            if band == '6':\n",
    "                                if filename.split(\"\\\\\")[-1].split(\"_\")[7:][2][0] == '1':\n",
    "                                    bnd = band\n",
    "                                else:\n",
    "                                    bnd = '9'\n",
    "                            elif band == 'Q':\n",
    "                                bnd = '10'\n",
    "                            \n",
    "                            sm = month + \"_B\" + bnd +\"_M\"\n",
    "                            \n",
    "                            cm = dictn[sm]\n",
    "                            \n",
    "                            r = r[0]\n",
    "                            # cm is the collumn indexe for mean\n",
    "                            # r[0] is the row index\n",
    "                            times[4] += timeit.default_timer() - st\n",
    "                            nums[4] += 1\n",
    "                            \n",
    "                            \n",
    "                            ##### Checking if values are null ...\n",
    "                            valm = ricex.iloc[r,cm]\n",
    "                            if pd.isnull(valm): \n",
    "                                \n",
    "                                st = timeit.default_timer()\n",
    "                                ricex.iloc[r,cm] = int(pix)\n",
    "                                ricex.iloc[r,cm+1] = int(pix*pix)\n",
    "                                ricex.iloc[r,cm+240] = 1\n",
    "                                times[5] += timeit.default_timer() - st\n",
    "                                nums[5] += 1\n",
    "                        \n",
    "                                continue\n",
    "                                \n",
    "                                \n",
    "                            st = timeit.default_timer()\n",
    "                            ##### if the values are not null ...\n",
    "                            valv = int(ricex.iloc[r,cm+1])\n",
    "                            n = int(ricex.iloc[r,cm+240])\n",
    "                            n = n+1\n",
    "                            times[6] += timeit.default_timer() - st\n",
    "                            nums[6] += 1\n",
    "                        \n",
    "                            \n",
    "                            \n",
    "                            st = timeit.default_timer()\n",
    "                            # Mean & Variance update\n",
    "                            ricex.iloc[r,cm] = valm + (pix-valm)/n\n",
    "                            ricex.iloc[r,cm+1] = ((n-2)/(n-1))*valv + (pix-valm)*(pix-valm)/n\n",
    "                            ricex.iloc[r,cm+240] = n\n",
    "                            \n",
    "                            times[7] += timeit.default_timer() - st\n",
    "                            nums[7] += 1\n",
    "                        \n",
    "                            \n",
    "                        \n",
    "                            #print (\"No match for the district \" + district + \" for the year \" + year)\n",
    "                            \n",
    "                            \n",
    "elapsed = timeit.default_timer() - stx\n",
    "print (elapsed)\n",
    "print \"Seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011475057348434348, 0.035648974422656465, 28.352838132581383, 58.64514798646269, 0.1363394056679681, 0.39012996186647797, 3.361048744081927, 30.0258021067566]\n",
      "[46818, 46818, 20916, 46521, 20916, 216, 20700, 20700]\n",
      "20916\n"
     ]
    }
   ],
   "source": [
    "print times\n",
    "print nums\n",
    "print hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.45099264138e-07\n",
      "1: 7.61437362182e-07\n",
      "2: 0.00135555737869\n",
      "3: 0.00126061666745\n",
      "4: 6.51842635628e-06\n",
      "5: 0.00180615723086\n",
      "6: 0.000162369504545\n",
      "7: 0.00145052184091\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    x = times[i]/nums[i]\n",
    "    print (str(i) + \": \" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
