{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Features from GeoTiff Files\n",
    "From GeoTiff Files available for India over a period of more than 20 years, we want to generate features from those files for the problem of prediction of district wise crop yield in India."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to gdal package, had to make a separate environment using conda. So install packages for this notebook in that environment itself. Check from the anaconda prompt, the names of all the envs are available: $conda info --envs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import ogr, osr, gdal\n",
    "\n",
    "import requests # To make the REST API call\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base = \"/Users/macbook/Documents/BTP/Satellite/Data/Sat\" # macOS\n",
    "base = \"G:\\BTP\\Satellite\\Data\\Test\"  # Win7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: G:\\BTP\\Satellite\\Data\\Test\n",
      "\tLE07_L1TP_146039_20101223_20161211_01_T1.tar.gz\n",
      "\tLE07_L1TP_146041_20101223_20161211_01_T1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "b = True\n",
    "for directory, subdirList, fileList in os.walk(base):\n",
    "#     if b:\n",
    "#         b = False\n",
    "#         continue\n",
    "    print (\"Directory: \" + directory)\n",
    "    for filename in fileList:\n",
    "        if filename[0] != '.': print (\"\\t\" + filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(os.path.join(base,root)) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s' % root)\n",
    "        tar = tarfile.open(os.path.join(base,filename))\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(os.path.join(base,root))\n",
    "        tar.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for LE07_L1TP_146039_20101223_20161211_01_T1\n",
      "Extracting data for LE07_L1TP_146041_20101223_20161211_01_T1\n"
     ]
    }
   ],
   "source": [
    "# extracting all the tar files ... (if not extracted)\n",
    "for directory, subdirList, fileList in os.walk(base):\n",
    "    for filename in fileList:\n",
    "        if filename.endswith(\".tar.gz\"): \n",
    "            d = extract(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directories = [os.path.join(base, d) for d in sorted(os.listdir(base)) if os.path.isdir(os.path.join(base, d))]\n",
    "# print directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the existing coordinate system\n",
    "old_cs= osr.SpatialReference()\n",
    "old_cs.ImportFromWkt(ds.GetProjectionRef())\n",
    "\n",
    "# create the new coordinate system\n",
    "wgs84_wkt = \"\"\"\n",
    "GEOGCS[\"WGS 84\",\n",
    "    DATUM[\"WGS_1984\",\n",
    "        SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
    "            AUTHORITY[\"EPSG\",\"7030\"]],\n",
    "        AUTHORITY[\"EPSG\",\"6326\"]],\n",
    "    PRIMEM[\"Greenwich\",0,\n",
    "        AUTHORITY[\"EPSG\",\"8901\"]],\n",
    "    UNIT[\"degree\",0.01745329251994328,\n",
    "        AUTHORITY[\"EPSG\",\"9122\"]],\n",
    "    AUTHORITY[\"EPSG\",\"4326\"]]\"\"\"\n",
    "new_cs = osr.SpatialReference()\n",
    "new_cs.ImportFromWkt(wgs84_wkt)\n",
    "\n",
    "# create a transform object to convert between coordinate systems\n",
    "transform = osr.CoordinateTransformation(old_cs,new_cs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pixel2coord(x, y, xoff, a, b, yoff, d, e):\n",
    "    \"\"\"Returns global coordinates from coordinates x,y of the pixel\"\"\"\n",
    "    xp = a * x + b * y + xoff\n",
    "    yp = d * x + e * y + yoff\n",
    "    return(xp, yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B3.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B4.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B5.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B6_VCID_1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B6_VCID_2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B7.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_B8.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146039_20101223_20161211_01_T1\\LE07_L1TP_146039_20101223_20161211_01_T1_BQA.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B3.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B4.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B5.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B6_VCID_1.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B6_VCID_2.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B7.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_B8.TIF\n",
      "G:\\BTP\\Satellite\\Data\\Test\\LE07_L1TP_146041_20101223_20161211_01_T1\\LE07_L1TP_146041_20101223_20161211_01_T1_BQA.TIF\n"
     ]
    }
   ],
   "source": [
    "for directory in directories:\n",
    "    \n",
    "    \"\"\" Identifying Month, Year, Spacecraft ID \"\"\"\n",
    "    date = directory.split('/')[-1].split('_')[3] # Change for Win7\n",
    "    satx = directory.split('/')[-1][3]\n",
    "    month = date[4:6]\n",
    "    year = date[0:4]\n",
    "    \n",
    "    \"\"\" Visiting every GeoTIFF file \"\"\" \n",
    "    for _,_,files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".TIF\"):\n",
    "                print os.path.join(directory,filename)\n",
    "                ds = gdal.Open(os.path.join(directory,filename))\n",
    "                if ds == None: continue\n",
    "                col, row, _ = ds.RasterXSize, ds.RasterYSize, ds.RasterCount\n",
    "                xoff, a, b, yoff, d, e = ds.GetGeoTransform()\n",
    "                \n",
    "                \"\"\" Now go to each pixel, find its lat,lon. Hence its district, and the pixel value \"\"\"\n",
    "                \"\"\" Find the row with same (Year,District), in Crop Dataset. \"\"\"\n",
    "                \"\"\" Find the feature using Month, Band, SATx \"\"\"\n",
    "                \"\"\" For this have to find Mean & Variance \"\"\"\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANDSAT 7,  MONTH: 12, YEAR: 2010\n",
      "['B1.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B2.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B3.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B4.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B5.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B6', 'VCID', '1.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B6', 'VCID', '2.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B7.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "['B8.TIF']\n",
      "Col:  16321,  Row: 14441\n",
      "['BQA.TIF']\n",
      "Col:   8161,  Row:  7221\n",
      "LANDSAT 7,  MONTH: 12, YEAR: 2010\n",
      "['B1.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B2.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B3.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B4.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B5.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B6', 'VCID', '1.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B6', 'VCID', '2.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B7.TIF']\n",
      "Col:   7931,  Row:  6961\n",
      "['B8.TIF']\n",
      "Col:  15861,  Row: 13921\n",
      "['BQA.TIF']\n",
      "Col:   7931,  Row:  6961\n"
     ]
    }
   ],
   "source": [
    "for directory in directories:\n",
    "    \n",
    "    \"\"\" Identifying Month, Year, Spacecraft ID \"\"\"\n",
    "    date = directory.split('\\\\')[-1].split('_')[3] # Change for Win7\n",
    "    satx = directory.split('\\\\')[-1][3]\n",
    "    month = date[4:6]\n",
    "    year = date[0:4]\n",
    "    \n",
    "    print \"LANDSAT {},  MONTH: {}, YEAR: {}\".format(satx,month,year)\n",
    "    \n",
    "    \"\"\" Visiting every GeoTIFF file \"\"\" \n",
    "    for _,_,files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".TIF\"):\n",
    "                print filename.split(\"\\\\\")[-1].split(\"_\")[7:]\n",
    "                ds = gdal.Open(os.path.join(directory,filename))\n",
    "                if ds == None: continue\n",
    "                col, row, _ = ds.RasterXSize, ds.RasterYSize, ds.RasterCount\n",
    "                xoff, a, b, yoff, d, e = ds.GetGeoTransform()\n",
    "                print \"Col: {0:6},  Row:{1:6}\".format(col,row)\n",
    "                \n",
    "                \"\"\" Now go to each pixel, find its lat,lon. Hence its district, and the pixel value \"\"\"\n",
    "                \"\"\" Find the row with same (Year,District), in Crop Dataset. \"\"\"\n",
    "                \"\"\" Find the feature using Month, Band, SATx \"\"\"\n",
    "                \"\"\" For this have to find Mean & Variance \"\"\"\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for LANDSAT 7:  col,row ~ **8000,7000**, with an exception of Band 8, with **16K,14K**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Pseudo Code\n",
    "---\n",
    "> Go to the base folder: extract every zip file, which is unextracted:\n",
    ">> For each folder present here:\n",
    ">>> For each tiff file (for each band):\n",
    ">>>> Identify the following:\n",
    "- Month, Year\n",
    "- District Name\n",
    "- Cloud Cover Percentage\n",
    "- Sat 7 or 8 (maybe from #files in the folder!\n",
    "\n",
    ">>>> According to SAT, meaning of bands change ...(Put them in corresponding features ...)\n",
    "\n",
    ">>>> Traverse every 100th pixel (for sat7 every Kth)\n",
    "\n",
    "----\n",
    "- *Month, Year, Spacecraft ID* all from the **File Name** itself\n",
    "- Regarding the pixel location selection:\n",
    "    - Either go for **definite points** at some gap and avg the **non zero ones**\n",
    "    - OR Can select the points **randomly** and avg the non zero ones only.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
